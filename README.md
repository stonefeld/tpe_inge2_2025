<h1 align="center">Ingenier칤a del Software II</h1>
<h3 align="center">POC: Apache Kafka</h3>
<h4 align="center">Primer cuatrimestre 2025</h4>

# Objetivo del proyecto

Este es una demo a modo de prueba de concepto que demuestra el funcionamiento de
_Apache Kafka_. Para esto planteamos un sistema que registra datos de
**temperatura** y **humedad** en distintas provincias y los env칤a a topics de
Kafka. Estos datos luego son consumidos por diferentes _consumers_. El proyecto
consiste en construir una peque침a arquitectura utilizando:

* _Apache Kafka_: a modo de plataforma de transmisi칩n de eventos por medio de
arquitectura _pub/sub_.
* _Zookeeper_: a modo de coordinador para registrar datos del topic y elegir la
partici칩n.
* _InfluxDB_: solo utilizado en una de las implementaciones del consumer para
poder guardar los datos le칤dos de _Kafka_ y luego visualizarlos con _Grafana_.
* _Grafana_: lo utilizamos para generar gr치ficos en tiempo real de lo que se
guarda en _InfluxDB_ de los datos tomados del topic de _Kafka_.

Para esto, dise침amos 3 scripts en Python que simulan diferentes entidades:

* `producer.py`: Simula la lectura de un sensor particular en una provincia
especificada. Se le especifican valores t칤picos de temperatura y humedad para la
provincia para simular una distribuci칩n _cercano a lo real_.
* `consumer.py`: Es un consumidor que se subscribe a los topics de Kafka y
guarda los datos le칤dos en la base de datos de _InfluxDB_. Esos datos est치n
pensados para poder ser [graficados en _Grafana_](#influxdb-y-grafana-consumer).
* `alert_consumer.py`: Es un consumidor al que se le especifica una provincia a
la que "prestar치 atenci칩n" y se le configuran valores de temperatura y humedad
y emite una alerta si los valores le칤dos de Kafka est치n por encima (o por
debajo si se lo especifica al _script_).

# Requisitos

* **Python:** El proyecto utiliza el toolchain de
[`uv`](https://docs.astral.sh/uv/getting-started/installation/). Aunque tambi칠n
puede correrse directo con Python versi칩n `3.12.9`
* **Docker**
* **Docker compose**

# Instalando las dependencias

Si se est치 utilizando `uv` como toolchain, correr los siguientes comandos dentro
del repositorio:

```bash
# Para crear y activar el entorno virtual
uv venv    # Esto tambi칠n instala la versi칩n de Python necesaria
source .venv/bin/activate  # En Unix
.venv\Scripts\activate     # En Windows

# Para instalar las dependencias
uv sync
```

# Levantando la arquitectura

Utilizando `docker-compose` podemos levantar la arquitectura mencionada en la
introducci칩n.

```bash
# Primero, para persistir las configuraciones de Grafana
docker volume create grafana-data

# Ahora si, podemos levantar la arquitectura
docker compose up --build -d
```

Esto levantar치 los siguientes contenedores:

| Contenedor | Puertos |
| --- | --- |
| `kafka` | `9092:9092` |
| `zookeeper` | `2181:2181` |
| `influxdb` | `8086:8086` |
| `grafana` | `3000:3000` |

# Corriendo

## Producers

Para correr los producers, se utiliza el script de `producer.py` que espera como
par치metros:

* `province_name`: El nombre de la provincia donde se simula la lectura del
sensor.
* `typical_temperature`: La temperatura t칤pica de la provincia, para simular
variaciones de temperatura cercanos a la realidad.
* `typical_humidity`: Idem anterior, pero con la humedad t칤pica de la provincia.

Entonces, podemos simular la lectura en varias provincias distintas corriendo lo
siguiente:

```bash
uv run producer.py 'Buenos Aires' 23 76
uv run producer.py 'Salta' 35 69
uv run producer.py 'Cordoba' 20 69
uv run producer.py 'Corrientes' 30 80
```

## Consumers

Existen dos consumers diferentes que pueden consumir los datos subscribi칠ndose a
los topics de Kafka

### InfluxDB y Grafana consumer

Este consumer tiene como objetivo leer los datos de Kafka y guardarlos en
InfluxDB para luego graficarlos en Grafana. Para correr este consumer, se puede
ejecutar los siguiente:

```bash
uv run consumer.py
```

Luego, estos datos se ir치n guardando en InfluxDB y, para poder visualizarlos en
Grafana tenemos que seguir las siguientes instrucciones:

1. Desde un browser, ir a la URL `http://localhost:3000`
2. Se pedir치 un usuario y contrase침a los cu치les por `admin` y `admin`
respectivamente.
3. Agregar un _Data Source_ a Grafana para InfluxDB (`http://localhost:3000/connections/datasources`):
    1. Seleccionar "_InfluxDB_"
    2. Para "_Query language_", seleccionar `Flux`
    3. Bajo `HTTP` y `URL` ingresar `http://influxdb:8086`
    4. Bajo "_Basic Auth Details_" ingresar en _User_ y _Password_, `admin` y
       `adminadmin`, respectivamente
    5. Bajo "_InfluxDB Details_"
       * _Organization_: `ar-edu-itba-inge2`
       * _Token_: `admin-token`
       * _Default Bucket_: `sensor-data`
4. Una vez configurado el _Data Source_, crear un nuevo _Dashboard_ para
   _InfluxDB_ y agregar 2 visualizaciones con las siguientes _queries_:

```flux
from(bucket: "sensor-data")
    |> range(start: -24h)
    |> filter(fn: (r) => r._measurement == "temperature")
    |> map(fn: (r) => ({ r with _field: r.ubicacion }))
```

```flux
from(bucket: "sensor-data")
    |> range(start: -24h)
    |> filter(fn: (r) => r._measurement == "humidity")
    |> map(fn: (r) => ({ r with _field: r.ubicacion }))
```

### Alert consumer

Por otro lado, tenemos el consumer que se ocupa de generar alertas en caso de
que una provincia particular alcance un cierto nivel de temperatura o humedad
por arriba (o por debajo). El script `alert_consumer.py` espera los siguientes
par치metros:

* `province`: La provincia a la cual monitoreamos su humedad y temperatura para
emitir la alerta.
* `temperature_threshold`: El l칤mite de temperatura que registra para emitir la
alerta.
* `humidity_threshold`: Idem anterior pero con la humedad.
* `alert_type`: Puede ser `above` o `below`. L칩gicamente, si este par치metro es
`above`, emite la alerta cuando los valores son mayores a su l칤mite
especificado. Caso contrario, si es `below`, se emite cuando los valores le칤dos
est치n por debajo.

Entonces, si queremos emitir una alerta cuando la temperatura en Buenos Aires es
mayor a 30춿C y su humedad mayor a 80%, podemos correr lo siguiente:

```bash
uv run alert_consumer.py 'Buenos Aires' 30 80 above
```

En caso de que se emiten las alertas se ver치n de la siguiente manera:

```
[2025-06-11 15:30:43] 游뚿 ALERTA DE TEMPERATURA: Buenos Aires above 30.0춿C (actual: 35.1춿C)
[2025-06-11 15:32:54] 游눦 ALERTA DE HUMEDAD: Buenos Aires above 80.0% (actual: 90.4%)
```
